# Real-Time Sign Language Recognition System

This project uses **MediaPipe**, **OpenCV**, and **LSTM-based deep learning** to recognize hand gestures (signs) from a webcam feed and classify them into predefined actions (like letters A, B, C). It captures hand landmarks using MediaPipe, processes them into structured data, trains a model, and performs real-time gesture recognition.

---

##  Key Features

-  Hand landmark detection using MediaPipe
-  Real-time gesture recognition via webcam
-  LSTM-based gesture classification model
-  Structured dataset collection (numpy-based)
-  Action mapping (e.g., A, B, C signs)
-  TensorBoard logging support for model training
-  Modular codebase with reusable `function.py`

---

## Project Structure
SignLanguageRecognition/
â”œâ”€â”€ collectdata.py # Record gesture data using webcam
â”œâ”€â”€ data.py # Convert images to keypoints numpy arrays
â”œâ”€â”€ trainmodel.py # Train LSTM model on the keypoints
â”œâ”€â”€ app.py # Run the real-time recognition system
â”œâ”€â”€ model.json # Saved model architecture
â”œâ”€â”€ model.h5 # Trained model weights
â”œâ”€â”€ function.py # All reusable helper functions
â”œâ”€â”€ MP_Data/ # Numpy array dataset (auto-generated)
â”œâ”€â”€ Image/ # Raw gesture image data (if any)
â”œâ”€â”€ Logs/ # TensorBoard training logs
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ðŸ“¦ Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/SignLanguageRecognition.git
   cd SignLanguageRecognition

Install dependencies:

bash
Copy
Edit
pip install -r requirements.txt


Usage Guide
ðŸ”¹ Step 1: Data Collection please download the Sign from any Open Source Image of A B C Of Hand Gesture
bash
Copy
Edit
python collectdata.py

Step 2: Convert Images to Numpy Arrays
bash
Copy
Edit
python data.py

: Train the LSTM Model
bash
Copy
Edit
python trainmodel.py


You can launch TensorBoard via:

bash
Copy
Edit
tensorboard --logdir=Logs/

Run Real-Time Prediction
bash
Copy
Edit
python app.py


Model Architecture
text
Copy
Edit
Input: 30 frames x 63 keypoints (21 hand landmarks Ã— 3 [x, y, z])
â†’ LSTM (64) â†’ LSTM (128) â†’ LSTM (64)
â†’ Dense (64) â†’ Dense (32)
â†’ Output: Softmax over actions (A, B, C)


 Example Output
Model prints real-time prediction like:

css
Copy
Edit
A
B
C

Requirements
Example requirements.txt:

txt
Copy
Edit
opencv-python
mediapipe
numpy
pandas
keras
tensorflow
scikit-learn

Install them via:

bash
Copy
Edit
pip install -r requirements.txt

Customization
Add more actions by updating:

python
Copy
Edit
actions = np.array(['A', 'B', 'C', 'Hello', 'Yes', 'No'])

